// src/app/features/evaluation/constants/benchmarks.ts
import { Beaker } from 'lucide-angular';
import {
  EVAL_MMLU,
  EVAL_HELLASWAG,
  EVAL_BIG_BENCH_HARD,
  EVAL_DROP,
  EVAL_TRUTHFUL_QA,
  EVAL_IFEVAL,
  EVAL_GSM_8K,
  EVAL_MATH_QA,
  EVAL_LOGIQA,
  EVAL_ARC,
  EVAL_WINNOGRANDE,
  EVAL_HUMANEVAL,
  EVAL_MBPP,
  EVAL_PIQA,
  EVAL_SIQA,
  EVAL_COMMONSENSEQA,
  EVAL_TRIVIAQA,
  EVAL_RACE,
  EVAL_PUBMEDQA,
  EVAL_SCIQ,
} from '../../tooltips';
import { MMLU_TASKS } from './mmlu';
import { BIG_BENCH_HARD_TASKS } from './bigbenchhard';

interface BenchmarkTask {
  label: string;
  value: string;
}

interface Benchmark {
  img: any;
  name: string;
  evalScopeName: string;
  type: string;
  tooltip: string;
  tasks: BenchmarkTask[] | null;
  defaultShots: number;
  supportsFewshot: boolean;
  defaultLimit?: number;
  minContextLength?: number;
}

export const BENCHMARKS: Benchmark[] = [
  {
    img: Beaker,
    name: 'MMLU',
    evalScopeName: 'mmlu',
    type: 'accuracy',
    tooltip: EVAL_MMLU,
    tasks: MMLU_TASKS,
    defaultShots: 5,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'HellaSwag',
    evalScopeName: 'hellaswag',
    type: 'accuracy',
    tooltip: EVAL_HELLASWAG,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
  },
  {
    img: Beaker,
    name: 'BIG-Bench Hard',
    evalScopeName: 'bbh',
    type: 'accuracy',
    tooltip: EVAL_BIG_BENCH_HARD,
    tasks: BIG_BENCH_HARD_TASKS,
    defaultShots: 3,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'DROP',
    evalScopeName: 'drop',
    type: 'accuracy',
    tooltip: EVAL_DROP,
    tasks: null,
    defaultShots: 3,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'TruthfulQA',
    evalScopeName: 'truthfulqa',
    type: 'accuracy',
    tooltip: EVAL_TRUTHFUL_QA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
  },
  {
    img: Beaker,
    name: 'IFEval',
    evalScopeName: 'ifeval',
    type: 'accuracy',
    tooltip: EVAL_IFEVAL,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
  },
  {
    img: Beaker,
    name: 'GSM8K',
    evalScopeName: 'gsm8k',
    type: 'accuracy',
    tooltip: EVAL_GSM_8K,
    tasks: null,
    defaultShots: 5,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'MathQA',
    evalScopeName: 'mathqa',
    type: 'accuracy',
    tooltip: EVAL_MATH_QA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
    defaultLimit: 100,
  },
  {
    img: Beaker,
    name: 'LogiQA',
    evalScopeName: 'logiqa',
    type: 'accuracy',
    tooltip: EVAL_LOGIQA,
    tasks: null,
    defaultShots: 1,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'ARC',
    evalScopeName: 'arc',
    type: 'accuracy',
    tooltip: EVAL_ARC,
    tasks: null,
    defaultShots: 25,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'Winogrande',
    evalScopeName: 'winogrande',
    type: 'accuracy',
    tooltip: EVAL_WINNOGRANDE,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
  },
  {
    img: Beaker,
    name: 'HumanEval',
    evalScopeName: 'humaneval',
    type: 'coding',
    tooltip: EVAL_HUMANEVAL,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
  },
  {
    img: Beaker,
    name: 'MBPP',
    evalScopeName: 'mbpp',
    type: 'coding',
    tooltip: EVAL_MBPP,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: false,
  },
  {
    img: Beaker,
    name: 'PIQA',
    evalScopeName: 'piqa',
    type: 'accuracy',
    tooltip: EVAL_PIQA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'SIQA',
    evalScopeName: 'siqa',
    type: 'accuracy',
    tooltip: EVAL_SIQA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'CommonsenseQA',
    evalScopeName: 'commonsenseqa',
    type: 'accuracy',
    tooltip: EVAL_COMMONSENSEQA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'TriviaQA',
    evalScopeName: 'triviaqa',
    type: 'accuracy',
    tooltip: EVAL_TRIVIAQA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
    minContextLength: 16000,
  },
  {
    img: Beaker,
    name: 'RACE',
    evalScopeName: 'race',
    type: 'accuracy',
    tooltip: EVAL_RACE,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'PubMedQA',
    evalScopeName: 'pubmedqa',
    type: 'accuracy',
    tooltip: EVAL_PUBMEDQA,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
  },
  {
    img: Beaker,
    name: 'SciQ',
    evalScopeName: 'sciq',
    type: 'accuracy',
    tooltip: EVAL_SCIQ,
    tasks: null,
    defaultShots: 0,
    supportsFewshot: true,
  },
];
